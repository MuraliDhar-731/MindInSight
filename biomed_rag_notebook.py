# -*- coding: utf-8 -*-
"""BioMed_RAG_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BX26vb6_Jlgan1nE3T75DxCZQ-mMt3o0
"""

!pip install tensorflow-datasets
!pip install transformers datasets sentencepiece faiss-cpu accelerate einops bert-score
!pip install sentence-transformers
!pip install sacremoses

import tensorflow_datasets as tfds
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForCausalLM,
    AutoModelForSeq2SeqLM, pipeline
)
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
import faiss
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt

from datasets import load_dataset
dataset = load_dataset("pubmed_qa", "pqa_labeled")
dataset

records = []

for item in dataset["train"]:
    q = item["question"]
    ctx = item["context"]["contexts"]  # list of passages
    gold = item["long_answer"]

    if len(ctx) == 0:
        continue

    records.append({
        "question": q,
        "snippets": ctx,
        "gold": gold
    })

len(records)

df = pd.DataFrame(records)
df.head()

corpus = []
for r in records:
    corpus.extend(r["snippets"])

len(corpus)

embed_model = SentenceTransformer("dmis-lab/biobert-base-cased-v1.1")

corpus_embeddings = embed_model.encode(
    corpus, batch_size=16, convert_to_numpy=True, show_progress_bar=True
).astype("float32")

dim = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(corpus_embeddings)

def retrieve(query, k=5):
    q_emb = embed_model.encode([query], convert_to_numpy=True).astype("float32")
    D, I = index.search(q_emb, k)
    return [corpus[i] for i in I[0]]

tok_biogpt = AutoTokenizer.from_pretrained("microsoft/biogpt")
model_biogpt = AutoModelForCausalLM.from_pretrained("microsoft/biogpt")

tok_flan = AutoTokenizer.from_pretrained("google/flan-t5-base")
model_flan = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

tok_phi = AutoTokenizer.from_pretrained("microsoft/phi-2")
model_phi = AutoModelForCausalLM.from_pretrained("microsoft/phi-2")

def build_prompt(question, passages):
    ctx = "\n\n".join(passages)
    return f"""

Context:
{ctx}

Question:
{question}

"""

def answer_biogpt(q, passages):
    prompt = build_prompt(q, passages)
    inputs = tok_biogpt(prompt, return_tensors="pt")
    out = model_biogpt.generate(**inputs, max_length=200)
    return tok_biogpt.decode(out[0], skip_special_tokens=True)

def answer_flan(q, passages):
    prompt = build_prompt(q, passages)
    inputs = tok_flan(prompt, return_tensors="pt")
    out = model_flan.generate(**inputs, max_length=200)
    return tok_flan.decode(out[0], skip_special_tokens=True)

def answer_phi(q, passages):
    prompt = build_prompt(q, passages)
    inputs = tok_phi(prompt, return_tensors="pt")
    out = model_phi.generate(**inputs, max_length=200)
    return tok_phi.decode(out[0], skip_special_tokens=True)

questions = df["question"].tolist()[:10]
questions

results = []

for q in tqdm(questions):
    passages = retrieve(q, k=1) # Reduced k from 5 to 1 to shorten the input prompt
    ans1 = answer_biogpt(q, passages)
    ans2 = answer_flan(q, passages)
    ans3 = answer_phi(q, passages)

    results.append({
        "question": q,
        "retrieved": passages,
        "BioGPT": ans1,
        "Flan-T5": ans2,
        "Phi-2": ans3
    })

results_df = pd.DataFrame(results)
results_df

from bert_score import score

def bert_f1(ref, pred):
    P, R, F = score([pred], [ref], lang="en", verbose=False)
    return float(F)

grader = pipeline("text2text-generation",
                  model="google/flan-t5-large",
                  tokenizer="google/flan-t5-large")

def factual_score(question, answer, context):
    prompt = f"""
Context:
{context}

Question:
{question}

Answer:
{answer}

Give a score from 0.0 to 1.0 for factual accuracy.
Only output a single number.
"""
    try:
        out = grader(prompt, max_length=10)[0]["generated_text"]
        return float(out)
    except:
        return 0.0

scores = []

for r in results:
    gold = r["retrieved"][0]

    for m in ["BioGPT", "Flan-T5", "Phi-2"]:
        ans = r[m]
        f1 = bert_f1(gold, ans)
        fact = factual_score(r["question"], ans, "\n".join(r["retrieved"]))

        scores.append({
            "question": r["question"],
            "model": m,
            "bert_f1": f1,
            "factual_accuracy": fact
        })

score_df = pd.DataFrame(scores)
score_df

avg_scores = score_df.groupby("model")[["bert_f1", "factual_accuracy"]].mean()
avg_scores

avg_scores.plot(kind="bar", figsize=(7,5))
plt.title("Model Comparison â€” PubMedQA RAG")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.show()