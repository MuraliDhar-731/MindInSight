# ğŸ“˜ MedInSight-RAG  
### Retrieval-Augmented Biomedical Question Answering  
*A Comparative Study of BioGPT, Flan-T5, and Phi-2*

---

## ğŸ§¬ Overview

**MedInSight-RAG** is a biomedical Retrieval-Augmented Generation (RAG) system designed to improve factual accuracy, reduce hallucinations, and enhance reasoning in biomedical question answering.

The project evaluates three Large Language Models:

- **BioGPT**  
- **Flan-T5**  
- **Phi-2**

All models are grounded using:

- **BioBERT embeddings**  
- **FAISS dense retrieval**  
- **Top-k PubMed passages**  
- **RAG-style prompt construction**

This ensures that generated answers are evidence-based, scientifically plausible, and more reliable for biomedical use cases.

---

## ğŸ¥ Why RAG for Biomedical QA?

Biomedical text is high-stakes and highly specialized.  
Traditional LLMs often:

- Hallucinate biological mechanisms  
- Misinterpret pathways  
- Generate unsupported claims  
- Confuse similar medical terminology  

**RAG mitigates these issues** by grounding model outputs in retrieved biomedical literature, improving both factual accuracy and reasoning.

---

## ğŸ—ï¸ System Architecture

The MedInSight-RAG pipeline includes:

1. **BioBERT** â†’ Convert passages into dense biomedical embeddings  
2. **FAISS** â†’ Perform similarity search over a 10k-passage PubMed corpus  
3. **Top-k Retrieval** â†’ Select evidence passages  
4. **RAG Prompting** â†’ Insert evidence into model prompt  
5. **LLM Answer Generation** â†’ BioGPT / Flan-T5 / Phi-2  
6. **Evaluation** â†’ Semantic similarity + factuality scoring  

**Architecture Flow**

![Architecture](https://github.com/user-attachments/assets/c498ccc2-a5c2-4c0d-b3d6-6d08af033c00)

---

## ğŸ“š Dataset

We use a curated subset of **PubMedQA**, containing:

- 1,000+ biomedical questions  
- Cleaned abstracts free of citation clutter  
- A 10k-passage retrieval corpus  
- Yes / No / Maybe reasoning labels  

PubMedQA is ideal for RAG because it emphasizes **fact-based biomedical reasoning**.

---

## ğŸ“Š Evaluation Metrics

### **Quantitative**
- **BERTScore F1** â€“ Semantic similarity  
- **Factuality scoring (Flan-T5 classifier)**  

### **Qualitative**
- Evidence usage  
- Biomedical plausibility  
- Hallucination severity  

---

## ğŸ§ª Results Summary

| Model   | Similarity | Accuracy | Composite |
|--------|------------|----------|-----------|
| BioGPT | 0.81       | 0.72     | 0.765     |
| **Flan-T5** | **0.87** | **0.83** | **0.85**  |
| Phi-2   | 0.78       | 0.69     | 0.735     |

### Key Findings

- **Flan-T5** â†’ Best overall factual grounding  
- **BioGPT** â†’ Highly fluent but more hallucinations  
- **Phi-2** â†’ Lightweight but lowest biomedical specificity  

---

## ğŸ”® Future Work

- Hybrid retrieval (dense + sparse)  
- Cross-encoder reranking  
- Fine-tuning models on biomedical evidence chains  
- Expansion to full PubMed corpus  

---

## ğŸ‘¥ Contributors

- **Muralidhar Kolimali** â€“ Embeddings, FAISS, model integration  
- **Sunaina Makkena** â€“ Dataset processing, evaluation  
- **Spandana Dammanagari** â€“ Model experimentation, qualitative analysis  

---

## ğŸ“– Citation

